{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb6cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb2af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 36001 files into human database\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# HUMANS IMAGES\n",
    "#########################\n",
    "\n",
    "humans_path = '/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/synthetic_images'\n",
    "\n",
    "all_humans = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        if not name.startswith(\".\"):\n",
    "            all_humans.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into human database\" % len(all_humans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4159a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 6000 files into all_humans_annotations database\n",
      "/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/annotations/female/subject_mesh_1968_anno.json\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# HUMANS ANNOTATIONS\n",
    "#########################\n",
    "\n",
    "humans_annotations_path = '/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/annotations'\n",
    "\n",
    "all_humans_annotations = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_annotations_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        if not name.startswith(\".\"):\n",
    "            all_humans_annotations.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into all_humans_annotations database\" % len(all_humans_annotations))\n",
    "\n",
    "print(all_humans_annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da1435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded:\n",
      "----------------------------------------------------\n",
      "Examples:\n",
      "----------------------------------------------------\n",
      "/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/synthetic_images/200x200/pose0/female/subject_mesh_1758-jeans.png\n",
      "/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/synthetic_images/200x200/pose0/female/subject_mesh_0978.png\n",
      "/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/synthetic_images/200x200/pose0/female/subject_mesh_0699-ein.png\n",
      "----------------------------------------------------\n",
      "Numbers:\n",
      "----------------------------------------------------\n",
      "12000\n",
      "12000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# DATA SORTING\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# HUMANS\n",
    "##############################\n",
    "\n",
    "humans_plain_paths = []\n",
    "humans_one_paths = []\n",
    "humans_jeans_paths = []\n",
    "\n",
    "for human in all_humans:\n",
    "    if \"ein\" in human:\n",
    "        humans_one_paths.append(human)\n",
    "    elif \"jeans\" in human:\n",
    "        humans_jeans_paths.append(human)\n",
    "    else:\n",
    "        humans_plain_paths.append(human)\n",
    "        \n",
    "##############################\n",
    "# ANNOTATIONS\n",
    "##############################\n",
    "\n",
    "humans_annotations_df = pd.DataFrame() \n",
    "\n",
    "for humans_annotation_path in all_humans_annotations:\n",
    "\n",
    "    if not \"DS_Store\" in humans_annotation_path:\n",
    "    \n",
    "        with open(humans_annotation_path, 'r') as file:\n",
    "            # load from json\n",
    "            human_annotation = json.load(file)\n",
    "            sub_df = pd.DataFrame.from_dict(human_annotation['human_dimensions'], orient='index')\n",
    "            sub_df = sub_df.transpose()\n",
    "\n",
    "            #humans_annotations_df = humans_annotations_df.append(sub_df, ignore_index=True)\n",
    "            humans_annotations_df = pd.concat([humans_annotations_df,sub_df])\n",
    "    \n",
    "##############################\n",
    "# OUTPUT\n",
    "##############################\n",
    "print(\"Images loaded:\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Examples:\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(humans_jeans_paths[0])\n",
    "print(humans_plain_paths[0])\n",
    "print(humans_one_paths[0])\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Numbers:\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(len(humans_jeans_paths))\n",
    "print(len(humans_plain_paths))\n",
    "humans_one_paths = humans_one_paths[:-1]\n",
    "print(len(humans_one_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e56d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['chest_circumference', 'height', 'inseam', 'left_arm_length',\n",
      "       'pelvis_circumference', 'right_arm_length', 'shoulder_width',\n",
      "       'waist_circumference'],\n",
      "      dtype='object')\n",
      "   chest_circumference    height    inseam  left_arm_length  \\\n",
      "0             1.200267  1.730106  0.684563         0.577781   \n",
      "0             0.973475  1.790180  0.841815         0.633797   \n",
      "0             0.748137  1.467417  0.628197         0.486339   \n",
      "0             0.882200  1.579298  0.649627         0.525917   \n",
      "0             1.016914  1.456891  0.612365         0.503487   \n",
      "\n",
      "   pelvis_circumference  right_arm_length  shoulder_width  waist_circumference  \n",
      "0              1.294256          0.597256        0.382759             1.103298  \n",
      "0              1.109804          0.635824        0.352502             0.814699  \n",
      "0              0.902817          0.473478        0.315147             0.609579  \n",
      "0              1.041371          0.528704        0.332407             0.711979  \n",
      "0              0.921696          0.492973        0.369244             0.829435  \n"
     ]
    }
   ],
   "source": [
    "print(humans_annotations_df.columns)\n",
    "print(humans_annotations_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6b9b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# CONVERT IMAGES TO GRAYSCALE\n",
    "##############################################################\n",
    "\n",
    "# reduce number of images for testing purposes\n",
    "n_images = 1000000\n",
    "\n",
    "humans_plain = []\n",
    "humans_jeans_grey = []\n",
    "humans_one_grey = []\n",
    "\n",
    "##############################\n",
    "# HUMANS PLAIN\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_plain_paths:\n",
    "    human_plain = io.imread(file_path)\n",
    "    \n",
    "    humans_plain.append(human_plain)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "print(\"1\")\n",
    "##############################\n",
    "# HUMANS W TEXTURE FOREGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_jeans_paths:\n",
    "    human_texture_grey = color.rgb2gray(io.imread(file_path))\n",
    "\n",
    "    humans_jeans_grey.append(human_texture_grey)\n",
    "\n",
    "    if i == n_images:\n",
    "        break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(\"2\")\n",
    "##############################\n",
    "# HUMANS W SOLID FOREGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_one_paths:\n",
    "    human_rgb_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "   \n",
    "    humans_one_grey.append(human_rgb_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "print(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e7d6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/xp7lnvk10sx90l166vcpd6p80000gn/T/ipykernel_14381/1774606740.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  humans_plain_image_tensor = torch.tensor(humans_plain)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humans_annotation_tensor.pt saved\n",
      "humans_plain_image_tensor.pt saved\n",
      "humans_one_image_tensor.pt saved\n",
      "humans_jeans_image_tensor.pt saved\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# CREATE DATA FRAME OF IMAGE VECTORS AND ANNOTATIONS\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# ANNOTATIONS\n",
    "##############################\n",
    "\n",
    "# tensor of annotations\n",
    "humans_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "humans_annotation_tensor = torch.cat((humans_annotation_tensor,humans_annotation_tensor),0)\n",
    "\n",
    "\n",
    "##############################\n",
    "# HUMAN PLAIN DATA FRAME\n",
    "##############################\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_plain_image_tensor = torch.tensor(humans_plain)\n",
    "\n",
    "##############################\n",
    "# HUMAN RGB DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_one_image_tensor = torch.tensor(humans_one_grey)\n",
    "\n",
    "##############################\n",
    "# HUMAN TEXTURE DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_jeans_image_tensor = torch.tensor(humans_jeans_grey)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# STORE TENSORS IN FILE\n",
    "############################################################\n",
    "\n",
    "##############################\n",
    "# PLAIN\n",
    "##############################\n",
    "\n",
    "torch.save(humans_annotation_tensor, '/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/tensors/humans_annotation_tensor.pt')\n",
    "print(\"humans_annotation_tensor.pt saved\")\n",
    "torch.save(humans_plain_image_tensor, '/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/tensors/humans_plain_image_tensor.pt')\n",
    "print(\"humans_plain_image_tensor.pt saved\")\n",
    "\n",
    "##############################\n",
    "# ONE COLOR\n",
    "##############################\n",
    "\n",
    "torch.save(humans_one_image_tensor, '/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/tensors/humans_one_image_tensor.pt')\n",
    "print(\"humans_one_image_tensor.pt saved\")\n",
    "\n",
    "##############################\n",
    "# JEANS\n",
    "##############################\n",
    "\n",
    "torch.save(humans_jeans_image_tensor, '/Users/daniilbarkov/MyProjects/Uni/Pattern2/dataset/tensors/humans_jeans_image_tensor.pt')\n",
    "print(\"humans_jeans_image_tensor.pt saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
